{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:05.607115Z","iopub.execute_input":"2022-07-14T15:53:05.608301Z","iopub.status.idle":"2022-07-14T15:53:06.368557Z","shell.execute_reply.started":"2022-07-14T15:53:05.608252Z","shell.execute_reply":"2022-07-14T15:53:06.367489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET = 'mimic_cxr'\nREPORT_TO = 'wandb'","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:06.370585Z","iopub.execute_input":"2022-07-14T15:53:06.371405Z","iopub.status.idle":"2022-07-14T15:53:06.376002Z","shell.execute_reply.started":"2022-07-14T15:53:06.371363Z","shell.execute_reply":"2022-07-14T15:53:06.374973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import annotations\n\nimport json\nimport random\nfrom collections import defaultdict\nfrom itertools import chain\nfrom pathlib import Path\n\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nfrom transformers import AutoModelForImageClassification, AutoFeatureExtractor, TrainingArguments, Trainer\n\nrandom.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:08.207294Z","iopub.execute_input":"2022-07-14T15:53:08.207686Z","iopub.status.idle":"2022-07-14T15:53:21.866466Z","shell.execute_reply.started":"2022-07-14T15:53:08.207657Z","shell.execute_reply":"2022-07-14T15:53:21.865084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label2id = {\n    'No Finding': 0,\n    'Atelectasis': 1,\n    'Cardiomegaly': 2,\n    'Consolidation': 3,\n    'Edema': 4,\n    'Enlarged Cardiomediastinum': 5,\n    'Fracture': 6,\n    'Lung Lesion': 7,\n    'Lung Opacity': 8,\n    'Pleural Effusion': 9,\n    'Pneumonia': 10,\n    'Pneumothorax': 11,\n    'Pleural Other': 12,\n    'Support Devices': 13,\n    'Other': 14,\n}\n\nid2label = {v: k for k, v in label2id.items()}\n\nlabels = list(label2id.keys())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:21.868326Z","iopub.execute_input":"2022-07-14T15:53:21.868961Z","iopub.status.idle":"2022-07-14T15:53:21.876716Z","shell.execute_reply.started":"2022-07-14T15:53:21.868928Z","shell.execute_reply":"2022-07-14T15:53:21.875449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class XrayReportDataset(Dataset):\n    \"\"\"Dataset class that contains the X-ray images and reports.\"\"\"\n\n    def __init__(\n        self,\n        splits: list[str],\n        image_dir: Path,\n        ann_path: Path,\n        id2label: dict,\n        label2id: dict,\n        transforms: AutoFeatureExtractor,\n        sample: float = 1.0\n    ):\n        \"\"\"Create train, validation and test datasets.\n\n        Args:\n            image_dir: path to directory of images\n            ann_path: path to annotations json\n            transforms: image transformations\n        \"\"\"\n        super().__init__()\n        if isinstance(splits, str):\n            splits = [splits]\n        \n        self.image_dir = image_dir\n        self.ann_path = ann_path\n        self.id2label = id2label\n        self.label2id = label2id\n        self.transforms = transforms\n        self.sample = sample\n        self.splits = splits\n\n        with open(self.ann_path, 'r') as f:\n            self.annotations = json.load(f)\n        \n        self.data = list(chain(*[\n            self.annotations[split]\n            for split in self.splits\n        ]))\n        if 0.0 < self.sample < 1.0:\n            total = max(int(len(self.data) * self.sample), 1)\n            self.data = random.sample(self.data, total)\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(\n        self, index: int\n    ):\n        \"\"\"\n        Retrieve an item from the dataset.\n        Parameters\n        ----------\n        index\n            dataset index\n        Returns\n        -------\n        ret\n            id of the image, transformed image, tokenized report, report attention mask\n        \"\"\"\n        item = self.data[index]\n        image = Image.open(self.image_dir / item['image_path'][0]).convert('RGB')\n        image_transformed = self.transforms(image, return_tensors='pt')\n        \n        label_names = item['labels']\n        if label_names == []:\n            label_names = ['Other']\n    \n        labels = torch.zeros(len(self.id2label), dtype=torch.float)\n        labels_ids = [self.label2id[l] for l in label_names]\n        \n        labels[labels_ids] = 1\n\n        return {\n            'labels_names': item['labels'],\n            'labels': labels,\n            'pixel_values': image_transformed['pixel_values'][0],\n        }\n\n\nclass XrayReportData:\n    \"\"\"DataModule class that contains the X-ray images and reports.\"\"\"\n\n    def __init__(\n        self,\n        image_dir: Path,\n        ann_path: Path,\n        id2label: dict,\n        label2id: dict,\n        transforms: AutoFeatureExtractor,\n        batch_size: int = 32,\n        sample: float = 1.0\n    ):\n        super().__init__()\n        self.image_dir = image_dir\n        self.ann_path = ann_path\n        self.transform = transforms\n        self.id2label = id2label\n        self.label2id = label2id\n        self.batch_size = batch_size\n        self.sample = sample\n        data = self._setup()\n        self.train = data['train']\n        self.validation = data[('val', 'test')]\n\n    def _setup(self):\n        \"\"\"Initialize the train, val and test datasets.\"\"\"\n        return {\n            split: XrayReportDataset(\n                split,\n                self.image_dir,\n                self.ann_path,\n                self.id2label,\n                self.label2id,\n                self.transform,\n                sample=self.sample\n            ) for split in  ['train', ('val', 'test')]\n        }","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:21.878367Z","iopub.execute_input":"2022-07-14T15:53:21.878713Z","iopub.status.idle":"2022-07-14T15:53:21.916815Z","shell.execute_reply.started":"2022-07-14T15:53:21.878681Z","shell.execute_reply":"2022-07-14T15:53:21.915584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch: dict) -> dict:\n    \"\"\"Collate function from data to model.\n\n    Args:\n        batch:\n            dataset batch\n\n    Returns:\n        pixel_values and labels\n    \"\"\"\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.stack([x['labels'] for x in batch])\n    }","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:21.919569Z","iopub.execute_input":"2022-07-14T15:53:21.919955Z","iopub.status.idle":"2022-07-14T15:53:21.934782Z","shell.execute_reply.started":"2022-07-14T15:53:21.919924Z","shell.execute_reply":"2022-07-14T15:53:21.933651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_trained_model = 'google/vit-base-patch16-224-in21k'\n\nimage_dir = Path(f'/kaggle/input/chestxraycaption/{DATASET}/{DATASET}/images')\nannotations = Path(f'/kaggle/input/chestxraycaption/{DATASET}/{DATASET}/annotation.json')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:21.936166Z","iopub.execute_input":"2022-07-14T15:53:21.936690Z","iopub.status.idle":"2022-07-14T15:53:21.945852Z","shell.execute_reply.started":"2022-07-14T15:53:21.936656Z","shell.execute_reply":"2022-07-14T15:53:21.944682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = AutoFeatureExtractor.from_pretrained(pre_trained_model)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:53:21.947156Z","iopub.execute_input":"2022-07-14T15:53:21.947473Z","iopub.status.idle":"2022-07-14T15:53:23.815029Z","shell.execute_reply.started":"2022-07-14T15:53:21.947446Z","shell.execute_reply":"2022-07-14T15:53:23.813866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = XrayReportData(\n    image_dir=image_dir,\n    ann_path=annotations,\n    id2label=id2label,\n    label2id=label2id,\n    transforms=transforms\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T16:19:02.633566Z","iopub.execute_input":"2022-07-14T16:19:02.633982Z","iopub.status.idle":"2022-07-14T16:19:10.258354Z","shell.execute_reply.started":"2022-07-14T16:19:02.633947Z","shell.execute_reply":"2022-07-14T16:19:10.256298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" model = AutoModelForImageClassification.from_pretrained(\n    pre_trained_model,\n    num_labels=len(labels),\n    id2label=id2label,\n    label2id=label2id,\n    problem_type='multi_label_classification'\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T16:05:47.173921Z","iopub.execute_input":"2022-07-14T16:05:47.174349Z","iopub.status.idle":"2022-07-14T16:06:01.476553Z","shell.execute_reply.started":"2022-07-14T16:05:47.174313Z","shell.execute_reply":"2022-07-14T16:06:01.475648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n\ndef compute_metrics(output):\n    predictions, references = output\n\n    predictions = (predictions == predictions.max(axis=1, keepdims=True)).astype('int32')\n    references = references.astype('int32')\n    references = references & predictions\n    \n    return {\n        'accuracy': accuracy_score(references, predictions),\n        'f1': f1_score(references, predictions, average='weighted', zero_division=0),\n        'precision': precision_score(references, predictions, average='weighted', zero_division=0),\n        'recall': recall_score(references, predictions, average='weighted', zero_division=0)\n    }","metadata":{"execution":{"iopub.status.busy":"2022-07-14T15:54:37.523776Z","iopub.execute_input":"2022-07-14T15:54:37.524208Z","iopub.status.idle":"2022-07-14T15:54:37.532463Z","shell.execute_reply.started":"2022-07-14T15:54:37.524175Z","shell.execute_reply":"2022-07-14T15:54:37.531106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\nargs = TrainingArguments(\n    output_dir='vit-model',\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    evaluation_strategy='steps',\n    save_strategy='steps',\n    save_steps=250,\n    eval_steps=250,\n    num_train_epochs=1,\n    logging_steps=50,\n    optim='adamw_torch',\n    learning_rate=2e-4,\n    save_total_limit=1,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    report_to=REPORT_TO,\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=data.train,\n    eval_dataset=data.validation,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n    tokenizer=transforms,\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T16:19:44.846192Z","iopub.execute_input":"2022-07-14T16:19:44.846661Z","iopub.status.idle":"2022-07-14T16:19:44.890687Z","shell.execute_reply.started":"2022-07-14T16:19:44.846617Z","shell.execute_reply":"2022-07-14T16:19:44.889257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if REPORT_TO == 'wandb':\n    import wandb\n    from kaggle_secrets import UserSecretsClient\n\n    user_secrets = UserSecretsClient()\n    WANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\n\n    wandb.login(key=WANDB_KEY)\n\n    wandb.init(\n        project=f\"chest-xray-classification-{DATASET}\",\n        config={\n            \"model\": json.loads(model.config.to_json_string()),\n            \"args\": json.loads(args.to_json_string())\n        }\n    )\n    wandb.run.name = f'multiclass-{wandb.run.name}'\n\n%env WANDB_LOG_MODEL=true","metadata":{"execution":{"iopub.status.busy":"2022-07-13T18:01:01.644133Z","iopub.execute_input":"2022-07-13T18:01:01.644585Z","iopub.status.idle":"2022-07-13T18:01:07.258134Z","shell.execute_reply.started":"2022-07-13T18:01:01.644553Z","shell.execute_reply":"2022-07-13T18:01:07.256793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T16:19:51.067565Z","iopub.execute_input":"2022-07-14T16:19:51.068000Z","iopub.status.idle":"2022-07-14T16:21:22.873142Z","shell.execute_reply.started":"2022-07-14T16:19:51.067961Z","shell.execute_reply":"2022-07-14T16:21:22.871877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-07-13T20:42:33.264817Z","iopub.execute_input":"2022-07-13T20:42:33.268709Z","iopub.status.idle":"2022-07-13T20:42:38.569594Z","shell.execute_reply.started":"2022-07-13T20:42:33.268660Z","shell.execute_reply":"2022-07-13T20:42:38.568446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
